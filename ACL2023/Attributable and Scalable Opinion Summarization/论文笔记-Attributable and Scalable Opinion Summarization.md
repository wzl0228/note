# Attributable and Scalable Opinion Summarization
[原论文地址](https://aclanthology.org/2023.acl-long.473.pdf)

## 评论聚合（意见摘要）面临的挑战
1. 很难获得参考摘要，因此模型训练总是再缺乏gold standard references基础上训练的（21年以前）
2. 热门产品（entities）可能有数百个评论，如果方法的可扩展性差，可能会造成计算困难
3. 我们期望得到的意见摘要应该是抽象的，剔除一些不必要的细节，但也不能产生包含虚假信息“幻觉”。理想情况下，模型应该是可归因的，提供一些证据来证明其输出（生成摘要）的合理性。

> 先前的工作一部分可归因可扩展，但是生成的摘要太具体（我们期望是剔除unnecessarily specific details），另一部分生成摘要太抽象包含幻觉，且缺乏扩展性。

## 本论文思想
生成抽象摘要的同时，附上对输入句子的引用，这些引用作为每个输出句子的证据验证输入评论的哪些部分被用于生成摘要。

方法具体为：
1. 将评论中自然语言句子编码为（在分层离散潜在空间的）路径。
2. 给定关于某个特定实体的多条评论语句，确定在这些评论之间共享的公共子路径（common subpaths），并将这些子路径解码回自然语言，生成摘要。这些包含（选出的）子路径（编码后）的句子，充当生成句子的证据。

![image](1.png)

训练（顶部）：将评论中的句子编码为（在分层离散潜在空间的）路径。图中将“Tasty burgers” 编码为（中间部分）“树”的单一路径，用黑色实线表示。

推理（底部）：对输入评论中的所有句子进行编码，并确定用于摘要的出现频次高的路径或子路径。图中将三个句子“Delicious burgers”，“The pizzas are great”，“The room 
was dirty”输入编码，聚合得到的一致意见（公共子路径）是“Good food”，图中红色显示的子路径是重复的（出现两次，频次最高），解码后应该会产生类似“Good food”的输出。

> 模型（HERCULES）是无监督的，训练过程中不需要参考摘要，而是依赖于模型引导的编码空间的特性。
>
> 可扩展（Scalable）：聚合过程发生在编码空间中，而不是长序列的token。
>
> 可归因（Attributable）：生成的摘要附带来自输入评论的支持性证据。
>
> 一定程度的可控性（controllability）：可以通过将聚合限制在与所需实体属性（方面）相关的子路径上，生成关注实体的特定方面（例如，位置）或情绪的摘要。

## 分层量化自动编码器（对应本论文思想1）
y是一个句子，代表一个tokens序列。

y的语义可以被编码为一组离散潜在变量（编码）q1:D∈[1,K]. 

q1:D按层次排序，q1表示句子的高层次语义信息（例如，方面或整体情绪）而qD表示句子细粒度的语义信息 （例如，使用的具体措辞或词语选择）。句子编码q1:D可以被视为通过层次结构或树的单一路径，如图1所示，其中树中的每个中间节点和叶节点对应于一个句子y（单一路径中，位置高的结点，是该句子高层次语义表示，位置低的结点，是该句子细粒度语义表示，对应的都是同一句子）。

### 概率模型
![image](2.png)

### 神经参数化
q1:D离散，但大多数神经方法在连续空间中操作。定义从编码器网络的输出z∈RD到q1:D的映射，对于解码器p(y|z)也一样。我们学习了一个码书（codebook）Cd∈RK×D，它将每个离散代码映射到一个连续嵌入Cd（qd）∈RD（Cd相当于一个词典，一个表，用一个矩阵表示q1:D）

![image](3.png)

q1:D旨在表示分层信息，因此每个层级编码上的分布是softmax分布，得分sd由从每个codebook embedding到（输入和所有先前级别的累积embeddings之间的残差）的L2距离得出。

![image](4.png)





## 在编码空间中聚合评论（对应本论文思想2）


