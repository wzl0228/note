# 项目微调模型实验记录

HuggingFace：
1. [t5_pegasus small](https://www.huggingface.co/imxly/t5-pegasus-small)
2. [t5_pegasus base](https://www.huggingface.co/imxly/t5-pegasus)
3. [Randeng-Pegasus-238M-Summary-Chinese](https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese)
4. [Randeng-Pegasus-523M-Summary-Chinese](https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese)
5. [ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b)
6. [ChatGLM2-6B（新出，还没上手用）](https://huggingface.co/THUDM/chatglm2-6b)

## 一、初步数据尝试
数据集“过滤”以后保留text文本不超过256长度的数据。（text的长度大于等于summary的长度）

数据集  | train | dev | test
---- | ----- | ----- | -----
数量  | 73 | 6 | 15
比例  | 0.78 | 0.06 | 0.16

### 1、使用t5_pegasus small进行finetune
参数量0.95亿
### 2、使用t5_pegasus base进行finetune
参数量2.75亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
t5_pegasus_finetune256  | 0.3707 | 0.2019 | 0.3137

### 3、使用Randeng-Pegasus-238M-Summary-Chinese进行finetune
参数量2.38亿
### 4、使用Randeng-Pegasus-523M-Summary-Chinese进行finetune
参数量5.23亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
Randeng-Pegasus-523M-Summary-Chinese_finetune256  | 0.3467 | 0.1831 | 0.2963

### 5、使用ChatGLM-6B进行ptuning
参数量60亿，batch_size=1，lr=2e-2，PRE_SEQ_LEN=256, max_source_length=max_target_length=256，step=500≈110个epoch

模型评估结果采用Rouge1、Rouge2、RougeL、Bleu-4评估，在训练过程中保留模型根据save_steps=500保存
评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
ChatGLM-6B_ptuning256  | 0.3958 | 0.2298 | 0.3405
### 6、使用ChatGLM-6B进行finetune
参数量60亿

## 二、评估指标对比
微调模型  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
t5_pegasus_finetune256  | 0.3707 | 0.2019 | 0.3137
Randeng-Pegasus-523M-Summary-Chinese_finetune256  | 0.3467 | 0.1831 | 0.2963
ChatGLM-6B_ptuning256  | 0.3958 | 0.2298 | 0.3405

目前ChatGLM-6B经过ptuning以后的指标最好，但是ChatGLM的epoch相比其他两个模型大的多。但t5_pegasus比Randeng效果好是大概率肯定的，因为微调的参数和方法完全一样。
