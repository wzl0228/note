# 项目微调模型实验记录

HuggingFace：
1. [t5_pegasus small](https://www.huggingface.co/imxly/t5-pegasus-small)
2. [t5_pegasus base](https://www.huggingface.co/imxly/t5-pegasus)
3. [Randeng-Pegasus-238M-Summary-Chinese](https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese)
4. [Randeng-Pegasus-523M-Summary-Chinese](https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese)
5. [ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b)
6. [ChatGLM2-6B（新出，还没上手用）](https://huggingface.co/THUDM/chatglm2-6b)

## 一、初步数据尝试
数据集“过滤”以后保留text文本不超过256长度的数据。（text的长度大于等于summary的长度）

数据集  | train | dev | test
---- | ----- | ----- | -----
数量  | 73 | 6 | 15
比例  | 0.78 | 0.06 | 0.16

### 1、使用t5_pegasus small进行finetune
参数量0.95亿
### 2、使用t5_pegasus base进行finetune
参数量2.75亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
t5_pegasus_finetune256  | 0.3707 | 0.2019 | 0.3137

### 3、使用Randeng-Pegasus-238M-Summary-Chinese进行finetune
参数量2.38亿
### 4、使用Randeng-Pegasus-523M-Summary-Chinese进行finetune
参数量5.23亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
Randeng-Pegasus-523M-Summary-Chinese_finetune256  | 0.3467 | 0.1831 | 0.2963

### 5、使用ChatGLM-6B进行ptuning
参数量60亿，batch_size=1，lr=2e-2，PRE_SEQ_LEN=256, max_source_length=max_target_length=256，step=500≈110个epoch

模型评估结果采用Rouge1、Rouge2、RougeL、Bleu-4评估，在训练过程中保留模型根据save_steps=500保存
评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
ChatGLM-6B_ptuning256  | 0.3958 | 0.2298 | 0.3405
### 6、使用ChatGLM-6B进行finetune
参数量60亿

### 7、初步数据的评估指标对比
微调模型  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
t5_pegasus_finetune256  | 0.3707 | 0.2019 | 0.3137
Randeng-Pegasus-523M-Summary-Chinese_finetune256  | 0.3467 | 0.1831 | 0.2963
ChatGLM-6B_ptuning256  | 0.3958 | 0.2298 | 0.3405

目前ChatGLM-6B经过ptuning以后的指标最好，但是ChatGLM的epoch相比其他两个模型大的多。但t5_pegasus比Randeng效果好是大概率肯定的，因为微调的参数和方法完全一样。

## 二、大批数据测试
### 1、0728数据预处理情况
表格/对应处理数据  | Q_blank | Q_long | Q_error | Q_lowRouge | Q_data | S_blank | S_lowRouge | S_error | S_data | S_draft=S_report
 ---- | ----- | ----- | ----- | ---- | ----- | ----- | ----- | ---- | ----- | ----- 
脱敏经济责任0717.xlsx  | 177 | 1144 | 2 | 203 | 4596 | 332 | 301 | 17 | 5472 | 1839 
脱敏网信安0725.xlsx  | 0 | 36 | 0 | 5 | 157 | 0 | 10 | 8 | 180 | 57
总计 | 177 | 1180 | 2 | 208 | **4753** | 332 | 311 | 25 | **5652** | 1896

lowRouge是以report作为参考，draft作为假设，计算rouge1/2/l的值，rouge1<10或rougel<5的被认为是lowRouge（即不好或标注有问题的数据），在rouge计算前使用的是jieba的精确模式做中文分词，如果直接用空格做分词会导致rouge计算的递归深度过大。

按照train：dev：test=8：1：1的比例分割数据集，具体情况如下：

数据集  | train | dev | test | text_len | summ_len
 ---- | ----- | ----- | ----- | ----- | -----
Ques  | 3802 | 475 | 476 | 5~729 | 3~460
Sugg  | 4521 | 565 | 566 | 3~1315 | 1~1301 

> 目前的数据还需要进一步处理 0802

### 2、0805数据最终处理情况
> 新增处理一：通过设置临界midRouge范围（观察该部分数据），调整rouge1/rougel对lowRouge数据剔除的阈值，最终调整为rouge1<12 rougel<7，临界阈值rouge1(13,15),rougel(8,10)
>
> 新增处理二：剔除完全相同的数据（两条不同数据的text和summary对应完全相同）
>
> 新增处理三：分割数据集，保证test数据集中包含各个长度范围的数据，保证测试的完整性。
>
> 新增处理四：手动剔除一些长度特别短的数据，eg：100.00万元。

问题描述  | Q_blank | Q_long | Q_error | Q_lowRouge | Q_midRouge | Q_copy | **Q_data **
 ---- | ----- | ----- | ----- | ---- | ----- | ----- | ----- 
data0728（两个表格合并）.xlsx  | 177 | 1179 | 25 | 346 | 102 | 701 | **3790**

整改意见  | S_blank | S_lowRouge | S_midRouge | S_error | S_copy | S_(darft=report) | **S_data **
 ---- | ----- | ----- | ----- | ---- | ----- | ----- | ----- 
data0728（两个表格合并）.xlsx  | 332 | 459 | 119 | 25 | 1211 | 1896 | **4174**

问题描述数据集 | all | train | dev | test | input_len | out_len | test_0_400 | test_400_800 | test_800_1315
 ---- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | -----
Ques  | 3790 | 3030 | 380 | 380 | 11~1315 | 8~1301 | 325 | 50 | 5 

整改意见数据集 | all | train | dev | test | input_len | out_len | test_0_400 | test_400_800 | test_800_1315
 ---- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | -----
Sugg  | 4174 | 3355 | 409 | 410 | 5~729 | 3~460 | 385 | 20 | 5 

### 3、0805数据微调模型情况

训练情况  | batch_size | epoch | max_input_len | max_output | 最大显存占用 | 训练时间（3090显卡）
---- | ----- | ----- | ----- | ----- | ----- | -----
t5_pegasus_Ques0805  | 1 | 10 | 1320 | 1320 | 13514MB=13.2GB | 1epoch(6+5min) 10epoch约2h
t5_pegasus_Sugg0805  | 1 | 10 | 800 | 500 | 7518MB=7.4GB | 约2h
ChatGLM-6B_Ques0805  | 1 | 3.17epoch=600step | 1320 | 1320 | 13230MB=13GB | 9h22min
ChatGLM-6B_Sugg0805  | 1 | 600step | 800 | 500 | 13010MB=13GB | 预估3h

问题描述评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
t5_pegasus_Ques0805  | 52.41 | 39.86 | 50.59
ChatGLM-6B_Ques0805  | 59.14 | 46.70 | 56.85

整改意见评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
t5_pegasus_Sugg0805  | 64.82 | 54.34 | 62.96 
ChatGLM-6B_Sugg0805  |  |  | 

rouge指标很高，和数据中大量存在text和summary相同有关

### 4、0805生成情况分析
问题描述：


整改意见：

### 5、0805待处理
> 多数据合并（text相同，summary是完全复制前一半内容和后一半内容），需要观察一下
>
> rouge的两个“过滤”处理：1. 输入的text和summary相差较大，剔除数据（目前已完成） 2. 生成的generate和summary相差较大（设定一个rouge的阈值），选择让模型重新生成（目前未完成），这种处理对t5_pegasus会不会无效？对chatglm是否有效？



