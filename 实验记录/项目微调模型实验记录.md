# 项目微调模型实验记录

HuggingFace：
1. [t5_pegasus small](https://www.huggingface.co/imxly/t5-pegasus-small)
2. [t5_pegasus base](https://www.huggingface.co/imxly/t5-pegasus)
3. [Randeng-Pegasus-238M-Summary-Chinese](https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese)
4. [Randeng-Pegasus-523M-Summary-Chinese](https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese)
5. [ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b)
6. [ChatGLM2-6B（新出，还没上手用）](https://huggingface.co/THUDM/chatglm2-6b)

## 一、【模型微调】初步数据尝试
数据集“过滤”以后保留text文本不超过256长度的数据。（text的长度大于等于summary的长度）

数据集  | train | dev | test
---- | ----- | ----- | -----
数量  | 73 | 6 | 15
比例  | 0.78 | 0.06 | 0.16

### 1、使用t5_pegasus small进行finetune
参数量0.95亿
### 2、使用t5_pegasus base进行finetune
参数量2.75亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
t5_pegasus_finetune256  | 0.3707 | 0.2019 | 0.3137

### 3、使用Randeng-Pegasus-238M-Summary-Chinese进行finetune
参数量2.38亿
### 4、使用Randeng-Pegasus-523M-Summary-Chinese进行finetune
参数量5.23亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
Randeng-Pegasus-523M-Summary-Chinese_finetune256  | 0.3467 | 0.1831 | 0.2963

### 5、使用ChatGLM-6B进行ptuning
参数量60亿，batch_size=1，lr=2e-2，PRE_SEQ_LEN=256, max_source_length=max_target_length=256，step=500≈110个epoch

模型评估结果采用Rouge1、Rouge2、RougeL、Bleu-4评估，在训练过程中保留模型根据save_steps=500保存
评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
ChatGLM-6B_ptuning256  | 0.3958 | 0.2298 | 0.3405
### 6、使用ChatGLM-6B进行finetune
参数量60亿

### 7、初步数据的评估指标对比
微调模型  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
t5_pegasus_finetune256  | 0.3707 | 0.2019 | 0.3137
Randeng-Pegasus-523M-Summary-Chinese_finetune256  | 0.3467 | 0.1831 | 0.2963
ChatGLM-6B_ptuning256  | 0.3958 | 0.2298 | 0.3405

目前ChatGLM-6B经过ptuning以后的指标最好，但是ChatGLM的epoch相比其他两个模型大的多。但t5_pegasus比Randeng效果好是大概率肯定的，因为微调的参数和方法完全一样。

## 二、【模型微调】大批数据测试
### 1、0728数据预处理情况
表格/对应处理数据  | Q_blank | Q_long | Q_error | Q_lowRouge | Q_data | S_blank | S_lowRouge | S_error | S_data | S_draft=S_report
 ---- | ----- | ----- | ----- | ---- | ----- | ----- | ----- | ---- | ----- | ----- 
脱敏经济责任0717.xlsx  | 177 | 1144 | 2 | 203 | 4596 | 332 | 301 | 17 | 5472 | 1839 
脱敏网信安0725.xlsx  | 0 | 36 | 0 | 5 | 157 | 0 | 10 | 8 | 180 | 57
总计 | 177 | 1180 | 2 | 208 | **4753** | 332 | 311 | 25 | **5652** | 1896

lowRouge是以report作为参考，draft作为假设，计算rouge1/2/l的值，rouge1<10或rougel<5的被认为是lowRouge（即不好或标注有问题的数据），在rouge计算前使用的是jieba的精确模式做中文分词，如果直接用空格做分词会导致rouge计算的递归深度过大。

按照train：dev：test=8：1：1的比例分割数据集，具体情况如下：

数据集  | train | dev | test | text_len | summ_len
 ---- | ----- | ----- | ----- | ----- | -----
Ques  | 3802 | 475 | 476 | 5~729 | 3~460
Sugg  | 4521 | 565 | 566 | 3~1315 | 1~1301 

> 目前的数据还需要进一步处理 0802

### 2、0805数据新增处理情况
> 新增处理一：通过设置临界midRouge范围（观察该部分数据），调整rouge1/rougel对lowRouge数据剔除的阈值，最终调整为rouge1<12 rougel<7，临界阈值rouge1(13,15),rougel(8,10)
>
> 新增处理二：剔除完全相同的数据（两条不同数据的text和summary对应完全相同）
>
> 新增处理三：分割数据集，保证test数据集中包含各个长度范围的数据，保证测试的完整性。
>
> 新增处理四：手动剔除一些长度特别短的数据，eg：100.00万元。
> 

问题描述  | Q_blank | Q_long | Q_error | Q_lowRouge | Q_midRouge | Q_copy | Q_data
 ---- | ----- | ----- | ----- | ---- | ----- | ----- | ----- 
data0728（两个表格合并）.xlsx  | 177 | 1179 | 25 | 346 | 102 | 701 | **3790**

整改意见  | S_blank | S_lowRouge | S_midRouge | S_error | S_copy | S_(darft=report) | S_data
 ---- | ----- | ----- | ----- | ---- | ----- | ----- | ----- 
data0728（两个表格合并）.xlsx  | 332 | 459 | 119 | 25 | 1211 | 1896 | **4174**

问题描述数据集 | all | train | dev | test | input_len | out_len | test_0_400 | test_400_800 | test_800_1315
 ---- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | -----
Ques  | 3790 | 3030 | 380 | 380 | 11~1315 | 8~1301 | 325 | 50 | 5 

整改意见数据集 | all | train | dev | test | input_len | out_len | test_0_400 | test_400_800 | test_800_1315
 ---- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | -----
Sugg  | 4174 | 3355 | 409 | 410 | 5~729 | 3~460 | 385 | 20 | 5 

### 3、0805数据微调模型情况

训练情况  | batch_size | epoch | max_input_len | max_output | 最大显存占用 | 训练时间（3090显卡）
---- | ----- | ----- | ----- | ----- | ----- | -----
t5_pegasus_Ques0805  | 1 | 10 | 1320 | 1320 | 13514MB=13.2GB | 1epoch(6+5min) 10epoch约2h
t5_pegasus_Sugg0805  | 1 | 10 | 800 | 500 | 7518MB=7.4GB | 约2h
ChatGLM-6B_Ques0805  | 1 | 3.17epoch=600step | 1320 | 1320 | 13230MB=13GB | 9h22min
ChatGLM-6B_Sugg0805  | 1 | 2.86epoch=600step | 800 | 500 | 13010MB=13GB | 4h16min

问题描述评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
t5_pegasus_Ques0805  | 52.41 | 39.86 | 50.59
ChatGLM-6B_Ques0805  | 59.14 | 46.70 | 56.85

整改意见评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
t5_pegasus_Sugg0805  | 64.82 | 54.34 | 62.96 
ChatGLM-6B_Sugg0805  | 65.20 | 54.89 | 63.43

rouge指标很高，和数据中大量存在text和summary相同有关

> 新增处理五：生成过程中，针对调用微调后模型生成的generata与summary计算rouge1或rougel低于30的gen，重新生成一次（后续可改为多次）

问题描述评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
t5_pegasus_Ques0805  | 52.41 | 39.86 | 50.59
t5_pegasus_Ques0805_regen  | 52.41 | 39.86 | 50.59
ChatGLM-6B_Ques0805  | 59.14 | 46.70 | 56.85
ChatGLM-6B_Ques0805_regen  | 60.51 | 47.84 | 58.31

整改意见评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
t5_pegasus_Sugg0805  | 64.82 | 54.34 | 62.96 
t5_pegasus_Sugg0805_regen  | 64.82 | 54.34 | 62.96 
ChatGLM-6B_Sugg0805  | 65.20 | 54.89 | 63.43
ChatGLM-6B_Sugg0805_regen  | 65.66 | 55.38 | 63.87

### 4、0805数据处理与生成情况问题分析
#### 问题描述数据：
1. 多数据合并存在问题（见5的多数据合并1/2），完全复制其中一部分的数据很少
2. 本身数据中存在不完整数据（eg：row_id 46,56），错误使用标点（eg：row_id 21,72），预估几十条以上
3. 筛选相同数据（data_copy）时，会漏掉一些长度不相同的数据（eg：row_id 88，94）或数据标注时数字部分标注时不一致的情况（eg：row_id 1881,1884），可以在判断复制的时候吧数字/字母/"万元"等标注可能不同的全部replace为空格，该处理只在判断复制时起作用，而不作为最终保存数据前的操作。
4. 极少量的XX，XXX没有剔除，但影响不大
5. 存在开头的“事实描述：”等（eg：row_id 6319）
6. 本身rouge剔除一些效果不好的数据，还会有遗留

#### 整改意见数据：
1. 极少量的XX没有剔除，影响不大，只有一个
2. text和summary相同的数据比例很大
3. 存在开头的“建议：”等（eg：row_id 806,809,936,2705）
4. 底稿意见不同，审计意见相同的数据标注（row_id 693,696），与之前约定的标注一致，方便做后续的意见总结
5. 本身rouge剔除一些效果不好的数据，还会有遗留

#### 问题描述生成：
1. 对text和summary相同的数据生成较好
2. 生成幻觉（Ques/test_result.json line 266）
3. 对于某些总结句较短的情况，趋向于生成总结句（Ques/test_result.json line 41,68,1589）
4. 对于特别短的某些句子，生成不稳定（Ques/test_result.json line 1517,1598）
5. 有些情况会趋向复制导致与标注的简短summary不一致（Ques/test_result.json line 1760）
6. 某些生成难度比较大（Ques/test_result.json line 2021）

#### 整改意见生成：
1. 某些生成难度比较大（Ques/test_result.json line 23）
2. 生成幻觉，生成的长度比text长（Ques/test_result.json line 32）
3. 有些情况会趋向复制导致与标注的简短summary不一致（Ques/test_result.json line 302）

#### 总结一下问题
1. 数据标注中本身数据问题、多数据合并问题、符号剔除问题
2. 生成数据中趋向复制问题（有利有弊）、生成幻觉问题、问题描述会出现生成较短的情况，整改意见会出现生成较长的情况（和训练数据有关）

### 5、待处理
> ❓有疑问⏳待完成：多数据合并1（text相同，summary是完全复制前一半内容和后一半内容，eg：row_id 40,41 71,72），需要观察一下，如果全部合并会不会模型生成时更倾向于复制？
>
> ❓有疑问⏳待完成：多数据合并2（text相同，summary不同，但并不是完全复制text其中一部分，eg: row_id 1681,1682 1879,1880,1881,1882,1884），需要观察一下，如果全部合并会不会模型生成时更倾向于复制？
>
> ❓有疑问⏳待完成：模型text和summary的相同的比例比较高，需不需要减少一部分这样的数据，以提升模型的生成能力，这样是否合理？
>
> ✅已完成：rouge的两个“过滤”处理进一步调整：1. 输入的text和summary相差较大，剔除数据（目前已完成） 2. 生成的generate和summary相差较大（设定一个rouge的阈值），选择让模型重新生成（目前已完成），这种处理对t5_pegasus会不会无效？**无效，试试beamsearch/采样❓**，对chatglm是否有效？**有效**。

### 6、0807待处理问题
1. ✅计算test测试结果的rouge分布，统计一下数量
2. ✅大规模测试，test数据按照长度分三个部分，test数据量调整在1000以上（选一些train/dev的数据）
3. 观察生成结果中：（1）看特别异常的情况（2） 看介于异常和复制之间的情况（3）生成难度比较高的样例，模型的生成情况
4. ✅输出测试结果时，计算两种rouge1/2/l的值，generate和text，generate和summary
5. generate的结果重新生成时，不能利用summary，这属于“作弊”操作，改为计算generate和text的rouge

### 7、0812&0813大量数据分析
使用chatglm训练模型ptuning/output/adgen-chatglm-6b-pt-Ques-256-2e-2/checkpoint-600和ptuning/output/adgen-chatglm-6b-pt-Sugg-256-2e-2/checkpoint-600，分别对问题描述生成和整改意见生成做测试。根据rouge-1/2/l指标对数据本身情况和生成情况进行分析，得到：

1. 问题描述生成数据分析0812v1.xlsx
2. 问题描述数据统计图表分析0812v1.doc
3. 整改意见生成数据分析0813v1.xlsx
4. 整改意见数据统计图表分析0813v1.doc
5. 整改意见底稿不同报告相同原数据整理0813v1
6. 标注整改意见底稿不同报告相同的数据.json

### 8、0823训练整改意见的两个模型

ptuning chatglm得到：

1. 长模型（long2short）：用底稿长于报告的数据和一部分底稿和报告相同数据训练
2. 短模型（short2long）：用底稿短于报告的数据和一部分底稿和报告相同数据训练

> 复制数据没有删减数量，按照底稿长于报告的数据、底稿短于报告的数据，这两部分数据的比例，分配底稿和报告相同的数据，这些数据的互相不交叉，所有数据合并在一起就是整改意见的所有可用数据。
>
> ✅在调用的时候，并没有真正使用两个模型，因为显存加载不下两个微调的chatglm，所以是使用一个模型，加载两个不同的prefix


模型/训练数据 | 总数 | 底稿长于报告 | 底稿短于报告 | 底稿和报告长度相同
---- | ----- | ----- | ----- | -----
long2short  | 2728 | 1790 | 0 | 938
short2long  | 1446 | 0 | 949 | 497

测试集为1000条数据（test_big.json）

整改意见评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
7中使用Sugg模型 | 65.86 | 55.23 | 62.96 
同时使用长模型和短模型+指标不好重新生成三次  | 68.28 | 58.48 | 66.39 

> 指标测试时，分别用长模型和短模型生成【意见1】和【意见2】，计算两个相对底稿的rouge指标，如果rouge1和rougeL都低于30，就重新生成三次取最好的rouge1+rougel的结果
>
> ✅在网页端，先生成【意见1】和【意见2】，计算rouge指标对应低于30，如果【意见1】指标低，就重新生成三次，得到【意见1-re】，如果【意见2】指标低，就重新生成三次，得到【意见2-re】。如果【意见1-re】与【意见1】相同，则不会显示出来，因为没有意义，【意见2-re】同理

### 9、0829训练多条合并模型
得到整改意见生成模型后，需要进一步训练多条整改意见的总结模型。由7的工作，目前筛选出898条数据是整改意见底稿不同报告相同的情况，而该部分数据本身就符合从底稿审计处理意见->整改意见->整改意见总结，因此我们直接用该部分数据训练，希望得到从底稿审计处理意见->整改意见总结的模型（相当于两阶段变一阶段）。

多条整改意见总结数据集 | all | train | dev | test | input_len | out_len
 ---- | ----- | ----- | ----- | ----- | ----- | ----- 
Sugg-sameReport  | 201 | 168 | 20 | 13 | 28~1033 | 4~223

多条整改意见总结评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
Sugg-sameReport | 37.18 | 21.11 | 34.13 
Sugg-sameReport+指标不好重新生成三次 | 42.71 | 26.44 | 39.27

## 三、【预训练+重新训练】
### 1、0902chatglm预训练-多条整改意见总结
具体做法：先用一部分问题概括中长度和多条整改意见总结201组长度范围内的文本，先训练一波，然后在这个训练得到的模型的基础上在训练多条合并的数据。
预训练数据集 | all | train | dev | test | input_len | out_len
 ---- | ----- | ----- | ----- | ----- | ----- | ----- 
Sugg-sameReport  | 2179 | 1936 | 243 | 0 | 28< 30~1031 <1033 | 4< 8~222 <223
### 2、0902t5的finetune-多条整改意见总结
多条整改意见总结评估指标*100 | R1 | R2 | RL
---- | ----- | ----- | ----- 
t5 pre_samereport0902(5 epoch) | 33.42 | 13.64 | 29.15
t5 pre_samereport0902(10 epoch)| | |
### 3、单条问题描述的数据剔除部分复制数据重新训练
### 4、多条t5的pegasus预训练

## 四、【部署】
### 1、待解决一：模型的离线下载/缓存，解决偶尔的连接中断问题
### 2、待解决二：模型的输入输出格式，输入输出调整为csv文件
### 3、待解决三：可尝试chatglm2去重新训练看结果是否有提升



