# 项目微调大模型实验记录
## 一、初步数据尝试
数据集“过滤”以后保留text文本不超过256长度的数据。（text的长度大于等于summary的长度）

数据集  | train | dev | test
---- | ----- | ----- | -----
数量  | 73 | 6 | 15
比例  | 0.78 | 0.06 | 0.16

### 1、使用t5_pegasus small进行finetune
参数量0.95亿
### 2、使用t5_pegasus base进行finetune
参数量2.75亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
t5_pegasus_fintune256  | 0.3707 | 0.2019 | 0.3137

### 3、使用Randeng-Pegasus-238M-Summary-Chinese进行finetune
参数量2.38亿
### 4、使用Randeng-Pegasus-523M-Summary-Chinese进行finetune
参数量5.23亿，batch_size=1，lr=2e-4，max_len=max_generate_len=256，epoch=20

模型评估结果采用Rouge1、Rouge2、RougeL评估，在训练过程中保留模型只采用RougeL指标

评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
Randeng-Pegasus-523M-Summary-Chinese_fintune256  | 0.3707 | 0.2019 | 0.3137

### 5、使用ChatGLM-6B进行ptuning
参数量60亿，batch_size=1，lr=2e-2，PRE_SEQ_LEN=256, max_source_length=max_target_length=256，step=500

模型评估结果采用Rouge1、Rouge2、RougeL、Bleu-4评估，在训练过程中保留模型根据save_steps=500保存
评估结果（保留小数点后4位）  | RG-1 | RG-2 | RG-L
---- | ----- | ----- | -----
ChatGLM-6B_fintune256  | 0.3707 | 0.2019 | 0.3137
### 6、使用ChatGLM-6B进行finetune
参数量60亿
